{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ef83f9",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 3 — Capstone: Build a Full AI Risk Strategy (Colab)\n",
    "\n",
    "This notebook guides you through building a **comprehensive AI Risk Strategy** for a simulated case study involving an AI-powered recruitment screening tool. You will:\n",
    "- Analyze the bias and governance risks in the scenario\n",
    "- Build a structured **risk strategy template** in Python\n",
    "- Classify controls across **Technical, Operational, Ethical, and Legal** domains\n",
    "- **Export** the strategy as YAML for documentation\n",
    "- **Visualize** coverage across domains using a bar chart\n",
    "- Reflect on priorities via an interactive prompt\n",
    "\n",
    "> **Source:** Lab 3: *Capstone: Build Full AI Risk Strategy* (Global Knowledge Training LLC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21258f5a",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1 — Set up your environment\n",
    "Run the cell below on **Google Colab**. It installs the required packages for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab, install dependencies.\n",
    "# (In Colab, matplotlib is available by default, but we ensure PyYAML is present.)\n",
    "!pip -q install pyyaml matplotlib\n",
    "print(\"Dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04ef97",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2 — Define the case scenario\n",
    "\n",
    "**Scenario summary:**  \n",
    "An AI recruitment screening tool ranks male candidates significantly higher than female candidates for similar profiles. The model is a **black-box** trained on **historical hiring data** and the system lacks **logging**, **monitoring**, and **bias mitigation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66627ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Capstone Case: AI Tool for Recruitment\n",
    "# Issue: Gender bias in screening results, lack of explainability,\n",
    "# absence of logging or risk assessment.\n",
    "\n",
    "scenario = {\n",
    "    \"title\": \"AI Recruitment Screening Tool\",\n",
    "    \"issues\": [\n",
    "        \"Gender bias in ranked outcomes (male > female for similar profiles)\",\n",
    "        \"Black-box model trained on historical hiring data\",\n",
    "        \"Lack of logging, monitoring, and documented bias mitigation\"\n",
    "    ],\n",
    "    \"context\": {\n",
    "        \"domain\": \"Hiring/HR\",\n",
    "        \"model_type\": \"Black-box (unspecified)\",\n",
    "        \"data_source\": \"Historical hiring data\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Scenario defined. Key issues:\")\n",
    "for i, issue in enumerate(scenario[\"issues\"], start=1):\n",
    "    print(f\"{i}. {issue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64a225",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3 — Build the strategy template\n",
    "We'll assemble a strategy across **four domains**:\n",
    "\n",
    "- **Technical**: engineering controls (testing, monitoring, explainability)  \n",
    "- **Operational**: lifecycle, versioning, incident response  \n",
    "- **Ethical**: fairness, inclusion, stakeholder engagement  \n",
    "- **Legal**: consent, transparency, regulatory compliance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad522ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a risk strategy template across key domains\n",
    "risk_strategy = {\n",
    "    \"Technical\": [\n",
    "        \"Fairness testing on sensitive attributes\",\n",
    "        \"Model drift detection and retraining\",\n",
    "        \"Implement explainable model alternatives\"\n",
    "    ],\n",
    "    \"Operational\": [\n",
    "        \"Version control for model and data\",\n",
    "        \"Data lineage and change tracking\",\n",
    "        \"Incident response plan for AI failure\"\n",
    "    ],\n",
    "    \"Ethical\": [\n",
    "        \"Bias audits using diverse datasets\",\n",
    "        \"Inclusive design during feature engineering\",\n",
    "        \"Stakeholder input for fairness criteria\"\n",
    "    ],\n",
    "    \"Legal\": [\n",
    "        \"Consent for data collection\",\n",
    "        \"Right-to-explanation mechanism for rejected candidates\",\n",
    "        \"Compliance with GDPR & EU AI Act\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display strategy\n",
    "for domain, items in risk_strategy.items():\n",
    "    print(f\"\\n{domain.upper()} CONTROLS:\")\n",
    "    for control in items:\n",
    "        print(f\" - {control}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243f042",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4 — Document the strategy in YAML\n",
    "We'll export both **metadata** and the **risk strategy** into a `final_risk_strategy.yaml` file for sharing and governance documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "metadata = {\n",
    "    \"project\": \"AI Recruitment Risk Strategy\",\n",
    "    \"created_by\": \"Your Name\",\n",
    "    \"date\": \"2025-10-25\",\n",
    "    \"description\": \"Risk strategy to address AI bias, transparency, and compliance issues.\"\n",
    "}\n",
    "\n",
    "with open(\"final_risk_strategy.yaml\", \"w\") as f:\n",
    "    yaml.dump({\"metadata\": metadata, \"risk_strategy\": risk_strategy}, f, sort_keys=False)\n",
    "\n",
    "print(\"Strategy successfully written to final_risk_strategy.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac10e7",
   "metadata": {},
   "source": [
    "\n",
    "## Task 5 — Visual summary using matplotlib\n",
    "This bar chart shows **how many controls** you have in each domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31640347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "domains = list(risk_strategy.keys())\n",
    "control_counts = [len(risk_strategy[d]) for d in domains]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(domains, control_counts)\n",
    "plt.title(\"AI Risk Strategy Coverage by Domain\")\n",
    "plt.ylabel(\"Number of Controls\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af51ba2",
   "metadata": {},
   "source": [
    "\n",
    "## Task 6 — Reflection and submission\n",
    "Reflect on the **most critical** domain for this case and explain **why**.\n",
    "\n",
    "> *Tip:* Consider where the **largest risk** currently lies (e.g., biased outcomes), what is **fastest to mitigate**, and what enables **ongoing governance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54303869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interactive reflection prompt\n",
    "try:\n",
    "    user_reflection = input(\"What is one risk domain you found most important and why? \")\n",
    "    print(f\"You highlighted: {user_reflection}\")\n",
    "except Exception as e:\n",
    "    print(\"If running as a batch (no stdin), skip this prompt. You can reflect in a markdown cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7b4cf",
   "metadata": {},
   "source": [
    "\n",
    "## Lab review — Knowledge check (single-choice)\n",
    "1. Why is **Fairness testing** placed under the **Technical** domain?  \n",
    "   A. It is required for documentation  \n",
    "   B. It improves model transparency  \n",
    "   C. It addresses ethical tradeoffs  \n",
    "   D. It involves technical metrics and model logic ✅\n",
    "\n",
    "2. What is a key **ethical** consideration in AI hiring tools?  \n",
    "   A. Whether drift detection exists  \n",
    "   B. Inclusion of legal disclaimers  \n",
    "   C. Use of inclusive and unbiased training data ✅  \n",
    "   D. Use of open-source libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368aea5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Appendix — How to download artifacts from Colab\n",
    "- The YAML file `final_risk_strategy.yaml` is created in the current working directory.  \n",
    "- In Colab, use the file browser (left sidebar) to **right‑click → Download**, or run:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "files.download(\"final_risk_strategy.yaml\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}